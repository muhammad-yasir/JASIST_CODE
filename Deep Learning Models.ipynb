{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import logging\n",
    "import sklearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./Data/Google Reviews.csv')  ## Google review dataset\n",
    "#df = pd.read_csv('./Data/Play Store App reviews.csv')  ## App reviews from Google Play Store\n",
    "\n",
    "df = df[~df['Final Coding'].isna()]\n",
    "X_data = df[['Review_Text']]\n",
    "y_data = df[['Final Coding']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomOverSampler(random_state= 42)\n",
    "x_rus ,y_rus = rus.fit_resample(X_data,y_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_rus, y_rus, test_size=0.3, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "eval_df = pd.DataFrame(X_test)\n",
    "eval_df['target'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlnet xlnet-base-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaaf910b9f44997acec706170d2e4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e817d9acb9eb4d7cad6a816e796f4304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dfab8fca854b0ab83dbfcb6884d827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee785a29c08444778c067a323328a784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2e7e616ed84cd0a04b63f2150b6e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d296d01c7b4f4be39d743678ba5d02c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b9a402eeef4eae87342913119f7a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d746ab80aa740ec98dac022ddd73df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# They are lot of arguments to play with\n",
    "'''\n",
    "args = {\n",
    "   'output_dir': 'outputs/',\n",
    "   'cache_dir': 'cache/',\n",
    "   'fp16': True,\n",
    "   'fp16_opt_level': 'O1',\n",
    "   'max_seq_length': 256,\n",
    "   'train_batch_size': 8,\n",
    "   'eval_batch_size': 8,\n",
    "   'gradient_accumulation_steps': 1,\n",
    "   'num_train_epochs': 3,\n",
    "   'weight_decay': 0,\n",
    "   'learning_rate': 4e-5,\n",
    "   'adam_epsilon': 1e-8,\n",
    "   'warmup_ratio': 0.06,\n",
    "   'warmup_steps': 0,\n",
    "   'max_grad_norm': 1.0,\n",
    "   'logging_steps': 50,\n",
    "   'evaluate_during_training': False,\n",
    "   'save_steps': 2000,\n",
    "   'eval_all_checkpoints': True,\n",
    "   'use_tensorboard': True,\n",
    "   'overwrite_output_dir': True,\n",
    "   'reprocess_input_data': False,\n",
    "}\n",
    "'''\n",
    "models = []\n",
    "models.append(('xlnet', 'xlnet-base-cased'))\n",
    "models.append(('roberta', 'roberta-base'))\n",
    "models.append(('bert', 'bert-base-uncased'))\n",
    "train_arg = {'num_train_epochs':4, 'train_batch_size':32, 'max_seq_length':128,'fp16': False,'output_dir': 'outputs/BERT/','overwrite_output_dir': True}\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    deep_learning_model = ClassificationModel(name, model, args=train_arg,use_cuda= False)\n",
    "    deep_learning_model.train_model(train_df)\n",
    "    y_pred = deep_learning_model.predict(eval_df['Review_Text'].values.tolist())\n",
    "    \n",
    "    print(\"Model is \",name)\n",
    "    print(\"Accuracy is \", accuracy_score(eval_df['target'],y_pred[0]))\n",
    "    print(\"Precision is \",precision_score(eval_df['target'],y_pred[0]))\n",
    "    print(\"Recall is \",recall_score(eval_df['target'],y_pred[0]))\n",
    "    print(\"F1-score is\",f1_score(eval_df['target'],y_pred[0]))\n",
    "    print(\"Mathew corrcoef is \",matthews_corrcoef(eval_df['target'],y_pred[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "919b6843753584bb5a6e833a429d8176522842f3c8b8fdcd54d0bec6bffbd095"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
